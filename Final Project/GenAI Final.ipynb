{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88dc8b5d",
   "metadata": {},
   "source": [
    "## **1. Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8314c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import gradio as gr\n",
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import pipeline\n",
    "from ranx import Qrels, Run, evaluate\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395d1c21",
   "metadata": {},
   "source": [
    "### **2. Paths & Data Loading**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d89d4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = r\"D:\\OneDrive\\Desktop\\Uni\\Year 3\\Semester 6\\Advanced Deep Learning Gen AI\\Final Project\"\n",
    "data_path = os.path.join(base_path, \"Datasets\")\n",
    "ai_file = os.path.join(data_path, \"top_ai_questions.json\")\n",
    "ds_file = os.path.join(data_path, \"top_datascience_questions.json\")\n",
    "\n",
    "with open(ai_file, \"r\") as f:\n",
    "    ai_data = json.load(f)\n",
    "with open(ds_file, \"r\") as f:\n",
    "    ds_data = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223b6b71",
   "metadata": {},
   "source": [
    "### **3. Build DataFrame**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a03e089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_df(data, domain):\n",
    "    return pd.DataFrame(\n",
    "        [\n",
    "            {\n",
    "                \"id\": item[\"question_id\"],\n",
    "                \"title\": item[\"title\"],\n",
    "                \"body\": item[\"body\"],\n",
    "                \"link\": item[\"link\"],\n",
    "                \"domain\": domain,\n",
    "            }\n",
    "            for item in data\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "df_ai = to_df(ai_data, \"AI\")\n",
    "df_ds = to_df(ds_data, \"DataScience\")\n",
    "df = pd.concat([df_ai, df_ds], ignore_index=True)\n",
    "df[\"full_text\"] = df[\"title\"] + \" \" + df[\"body\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787864fb",
   "metadata": {},
   "source": [
    "### **4. BM25 Search Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6223bf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(text):\n",
    "    return [token.text.lower() for token in nlp(text)]\n",
    "\n",
    "\n",
    "tokenized_corpus = [spacy_tokenize(doc) for doc in df[\"full_text\"]]\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87d603ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_bm25(query, top_k=10):\n",
    "    tok_q = spacy_tokenize(query)\n",
    "    scores = bm25.get_scores(tok_q)\n",
    "    indices = scores.argsort()[-top_k:][::-1]\n",
    "    return df.iloc[indices][[\"id\", \"title\", \"link\", \"domain\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b30e1a",
   "metadata": {},
   "source": [
    "### **5. Semantic Search Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9383830",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "corpus = df[\"full_text\"].tolist()\n",
    "corpus_embeddings = model.encode(corpus, convert_to_tensor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fccaf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_semantic(query, top_k=10):\n",
    "    q_emb = model.encode(query, convert_to_tensor=True)\n",
    "    hits = util.semantic_search(q_emb, corpus_embeddings, top_k=top_k)[0]\n",
    "    ids = [hit[\"corpus_id\"] for hit in hits]\n",
    "    return df.iloc[ids][[\"id\", \"title\", \"link\", \"domain\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9c31c7",
   "metadata": {},
   "source": [
    "### **6. Hybrid (RRF Search)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6427b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query, top_k=10, rrf_k=60):\n",
    "    bm25_df = search_bm25(query, top_k)\n",
    "    sem_df = search_semantic(query, top_k)\n",
    "\n",
    "    bm25_ids = bm25_df[\"id\"].apply(lambda x: df[df[\"id\"] == x].index[0]).tolist()\n",
    "    sem_ids = sem_df[\"id\"].apply(lambda x: df[df[\"id\"] == x].index[0]).tolist()\n",
    "\n",
    "    scores = {}\n",
    "    for rank, idx in enumerate(bm25_ids):\n",
    "        scores[idx] = scores.get(idx, 0) + 1 / (rrf_k + rank + 1)\n",
    "    for rank, idx in enumerate(sem_ids):\n",
    "        scores[idx] = scores.get(idx, 0) + 1 / (rrf_k + rank + 1)\n",
    "\n",
    "    sorted_ids = sorted(scores, key=lambda i: scores[i], reverse=True)[:top_k]\n",
    "    return df.iloc[sorted_ids][[\"id\", \"title\", \"link\", \"domain\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dec929e",
   "metadata": {},
   "source": [
    "### **7. Build Ground Truth & Run Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1422607",
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = {\n",
    "    \"What is the difference between AI and machine learning?\": [35],\n",
    "    \"What is a convolutional neural network?\": [5546],\n",
    "    \"How is overfitting handled in ML?\": [61],\n",
    "    \"What are the main applications of reinforcement learning?\": [3502],\n",
    "    \"What are class weights in Keras and why use them?\": [13490],\n",
    "}\n",
    "\n",
    "ground_truth_str = {\n",
    "    q: {str(doc_id): 1 for doc_id in docs} for q, docs in ground_truth.items()\n",
    "}\n",
    "\n",
    "sample_queries_for_eval = list(ground_truth.keys())\n",
    "\n",
    "bm25_results_dict = {q: search_bm25(q, top_k=10) for q in sample_queries_for_eval}\n",
    "semantic_results_dict = {\n",
    "    q: search_semantic(q, top_k=10) for q in sample_queries_for_eval\n",
    "}\n",
    "hybrid_results_dict = {q: hybrid_search(q, top_k=10) for q in sample_queries_for_eval}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68b5a18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_run_file(results_dict):\n",
    "    run_file = {}\n",
    "    for query, result_df in results_dict.items():\n",
    "        run_file[query] = {\n",
    "            str(result_df.iloc[i][\"id\"]): 1 / (i + 1) for i in range(len(result_df))\n",
    "        }\n",
    "    return run_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a348b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_run_file = create_run_file(bm25_results_dict)\n",
    "semantic_run_file = create_run_file(semantic_results_dict)\n",
    "hybrid_run_file = create_run_file(hybrid_results_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0f8f8",
   "metadata": {},
   "source": [
    "### **8. Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41261adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric             BM25   Semantic     Hybrid\n",
      "----------------------------------------------\n",
      "map@10           0.2722     0.7333     0.7500\n",
      "mrr@10           0.2722     0.7333     0.7500\n",
      "ndcg@10          0.3987     0.7974     0.8123\n"
     ]
    }
   ],
   "source": [
    "qrels = Qrels(ground_truth_str)\n",
    "bm25_run = Run(bm25_run_file)\n",
    "semantic_run = Run(semantic_run_file)\n",
    "hybrid_run = Run(hybrid_run_file)\n",
    "\n",
    "bm25_metrics = evaluate(qrels, bm25_run, metrics=[\"map@10\", \"mrr@10\", \"ndcg@10\"])\n",
    "hybrid_metrics = evaluate(qrels, hybrid_run, metrics=[\"map@10\", \"mrr@10\", \"ndcg@10\"])\n",
    "semantic_metrics = evaluate(\n",
    "    qrels, semantic_run, metrics=[\"map@10\", \"mrr@10\", \"ndcg@10\"]\n",
    ")\n",
    "\n",
    "print(f\"{'Metric':<12} {'BM25':>10} {'Semantic':>10} {'Hybrid':>10}\")\n",
    "print(\"-\" * 46)\n",
    "for metric in [\"map@10\", \"mrr@10\", \"ndcg@10\"]:\n",
    "    bm25_val = bm25_metrics[metric]\n",
    "    semantic_val = semantic_metrics[metric]\n",
    "    hybrid_val = hybrid_metrics[metric]\n",
    "    print(f\"{metric:<12} {bm25_val:>10.4f} {semantic_val:>10.4f} {hybrid_val:>10.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107eb806",
   "metadata": {},
   "source": [
    "**Interpretation**:\n",
    "- **BM25** performs poorly on its own, likely due to strict lexical matching limitations.\n",
    "- **Semantic search** significantly improves MAP and MRR by capturing semantic similarity via dense embeddings.\n",
    "- **Hybrid search (BM25 + Semantic via RRF)** outperforms both individual methods, showing the benefit of combining lexical and semantic signals.\n",
    "\n",
    "These results validate the use of hybrid retrieval in our QA system, especially for generating accurate and contextually grounded answers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16ed884",
   "metadata": {},
   "source": [
    "### **9. RAG with HuggingFace T5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4612dc49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "qa_model = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c58cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_rag_hf(query, top_k=5):\n",
    "    hits = hybrid_search(query, top_k=top_k)\n",
    "    passages = []\n",
    "    for _, row in hits.iterrows():\n",
    "        title = row[\"title\"]\n",
    "        body = df[df[\"id\"] == row[\"id\"]][\"body\"].values[0]\n",
    "        passages.append(f\"{title}. {body}\")\n",
    "\n",
    "    numbered = \"\\n\\n\".join(f\"[{i+1}] {p}\" for i, p in enumerate(passages))\n",
    "\n",
    "    prompt = (\n",
    "        \"Use the following passages (with bracketed labels) to answer the question. \"\n",
    "        \"Include inline citations like [1], [2], etc.\\n\\n\"\n",
    "        f\"{numbered}\\n\\n\"\n",
    "        f\"Question: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    output = qa_model(prompt, max_new_tokens=256)[0][\"generated_text\"]\n",
    "    return output, hits[\"link\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "624367b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p>Can someone explain to me the difference between machine learning and deep learning? Is it possible to learn deep learning without knowing machine learning?/p> [2] What is the difference between self-supervised and unsupervised learning?. p> What is the difference between a href=\"https://ai.stackexchange.com/questions/10623/what-is-self-supervised-learning-in-machine-learning?noredirect=1&amp;lq=1\">self-supervised/a> and unsupervised learning? The terms logically overlap (and maybe self-supervised learning is a subset of unsupervised learning?), but I cannot pinpoint exactly what that difference is./p> [3] Difference between machine learning and artificial intelligence. p>Is there any difference between machine learning and artificial intelligence? Or do these terms refer to the same thing?/p> [4] What does AI software look like, and how is it different from other software?. p> What does AI software look like? What is the major difference between AI software and other software?/\n",
      "['https://ai.stackexchange.com/questions/1742/what-is-the-difference-between-machine-learning-and-deep-learning', 'https://ai.stackexchange.com/questions/40341/what-is-the-difference-between-self-supervised-and-unsupervised-learning', 'https://datascience.stackexchange.com/questions/19077/difference-between-machine-learning-and-artificial-intelligence', 'https://ai.stackexchange.com/questions/16448/what-does-ai-software-look-like-and-how-is-it-different-from-other-software', 'https://ai.stackexchange.com/questions/35/what-is-the-difference-between-artificial-intelligence-and-machine-learning']\n"
     ]
    }
   ],
   "source": [
    "ans, cites = generate_answer_rag_hf(\n",
    "    \"What is the difference between deep learning and AI?\"\n",
    ")\n",
    "print(ans)\n",
    "print(cites)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8da1ea",
   "metadata": {},
   "source": [
    "### **10. Gradio Interface for Live Q&A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d59a183a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_qa(query):\n",
    "    answer, cites = generate_answer_rag_hf(query, top_k=5)\n",
    "    df_cites = pd.DataFrame({\"source_link\": cites})\n",
    "    return answer, df_cites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f421bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\PC\\anaconda3\\Lib\\site-packages\\gradio\\interface.py:419: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7867/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo = gr.Interface(\n",
    "    fn=hybrid_qa,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=2,\n",
    "        placeholder=\"Ask any AI or data science question...\",\n",
    "        label=\"Your Question\",\n",
    "    ),\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Generated Answer\", lines=6),\n",
    "        gr.Dataframe(label=\"Top Retrieved Sources (Title, Link, Domain)\"),\n",
    "    ],\n",
    "    title=\"💡 Hybrid Q&A with BM25 + MiniLM + FLAN-T5\",\n",
    "    description=(\n",
    "        \"This tool retrieves relevant questions from a combined AI and Data Science dataset using \"\n",
    "        \"hybrid search (BM25 + MiniLM embeddings), then generates a cited answer using FLAN-T5. \"\n",
    "        \"Inline citations like [1], [2] refer to the listed sources below.\"\n",
    "    ),\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "demo.launch(quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70c90706",
   "metadata": {},
   "source": [
    "### **11. Q&A Generation Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cd85131b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer_hf(query, top_k=5):\n",
    "    hits = hybrid_search(query, top_k)\n",
    "    context_passages = []\n",
    "    for _, row in hits.iterrows():\n",
    "        body = df[df[\"id\"] == row[\"id\"]][\"body\"].values[0]\n",
    "        context_passages.append(f\"{row['title']}. {body}\")\n",
    "\n",
    "    context = \"\\n\\n\".join(context_passages)\n",
    "    prompt = (\n",
    "        f\"Use the context below to answer the question as accurately as possible.\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\n\"\n",
    "        f\"Question: {query}\\nAnswer:\"\n",
    "    )\n",
    "\n",
    "    ans = qa_model(prompt, max_new_tokens=256, do_sample=False)[0][\"generated_text\"]\n",
    "    return ans.strip(), hits[\"link\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a6e2459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      " Deep learning is a software that builds a system for the other.\n",
      "\n",
      "Sources:\n",
      "- https://ai.stackexchange.com/questions/1742/what-is-the-difference-between-machine-learning-and-deep-learning\n",
      "- https://ai.stackexchange.com/questions/40341/what-is-the-difference-between-self-supervised-and-unsupervised-learning\n",
      "- https://datascience.stackexchange.com/questions/19077/difference-between-machine-learning-and-artificial-intelligence\n",
      "- https://ai.stackexchange.com/questions/16448/what-does-ai-software-look-like-and-how-is-it-different-from-other-software\n",
      "- https://ai.stackexchange.com/questions/35/what-is-the-difference-between-artificial-intelligence-and-machine-learning\n"
     ]
    }
   ],
   "source": [
    "answer, cites = generate_answer_hf(\n",
    "    \"What is the difference between deep learning and AI?\"\n",
    ")\n",
    "print(\"Answer:\\n\", answer)\n",
    "print(\"\\nSources:\\n\" + \"\\n\".join(f\"- {u}\" for u in cites))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
